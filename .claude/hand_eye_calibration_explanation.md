# Hand-Eye Calibration for GelSight Data Collection

## Table of Contents
1. [Why We Need This Calibration](#why-we-need-this-calibration)
2. [What We're Computing](#what-were-computing)
3. [The Mathematical Framework](#the-mathematical-framework)
4. [How We Obtain Each Transform](#how-we-obtain-each-transform)
5. [The Complete Workflow](#the-complete-workflow)
6. [Tools and Methods](#tools-and-methods)

---

## Why We Need This Calibration

### The Problem
During GelSight tactile data collection:
- **GelSight sensor** is mounted on the robot gripper and collects tactile images when touching objects
- **RealSense camera** is also mounted on the gripper and captures RGB images of the scene
- **Robot** reports its gripper position in the robot base frame

### What We Need to Record for Each Data Sample
For YCB-Sight type datasets, each sample needs:
1. GelSight tactile image (what the sensor feels)
2. RealSense RGB image (what the camera sees)
3. Robot pose (where the gripper is)
4. **GelSight center position in robot base coordinates** â† This requires calibration!
5. **Contact location on the object** (where GelSight is touching)
6. **Surface normal at contact point**

### Why We Can't Get This Without Calibration
The robot only knows where the **gripper** is, not where the **GelSight sensor** is.

```
ROBOT BASE
    â†“ (robot knows this)
GRIPPER
    â†“ (??? unknown without calibration)
CAMERA
    â†“ (??? unknown without calibration)
GELSIGHT SENSOR
```

**Without calibration:** We have tactile images but don't know where in 3D space they were taken!

**With calibration:** We can compute the exact 3D position and orientation of the GelSight sensor for every data sample.

---

## What We're Computing

### The Goal
Compute the GelSight sensor position in robot base coordinates:

```
T_{baseâ†’gelsight} = Position and orientation of GelSight in robot base frame
```

### The Chain of Transforms
```
BASE â†’ GRIPPER â†’ CAMERA â†’ GELSIGHT

T_{baseâ†’gelsight} = T_{baseâ†’ee} Ã— T_{eeâ†’camera} Ã— T_{cameraâ†’gelsight}
                        â†‘              â†‘                  â†‘
                   (robot FK)    (calibrate!)       (calibrate!)
```

---

## The Mathematical Framework

### Homogeneous Transformation Matrix

A transformation matrix **T** represents both position and orientation:

```
T = [R  |  t]
    [0áµ€ |  1]
```

Where:
- **R** = 3Ã—3 rotation matrix (orientation)
- **t** = 3Ã—1 translation vector (position)
- **0áµ€** = [0, 0, 0]

Full 4Ã—4 form:
```
    [râ‚â‚  râ‚â‚‚  râ‚â‚ƒ  tâ‚“]
T = [râ‚‚â‚  râ‚‚â‚‚  râ‚‚â‚ƒ  táµ§]
    [râ‚ƒâ‚  râ‚ƒâ‚‚  râ‚ƒâ‚ƒ  táµ¤]
    [ 0    0    0   1 ]
```

### Matrix Multiplication Rule

For two transforms Tâ‚ and Tâ‚‚:
```
Tâ‚ Â· Tâ‚‚ = [Râ‚Â·Râ‚‚  |  Râ‚Â·tâ‚‚ + tâ‚]
          [ 0áµ€    |      1     ]
```

---

## Mathematical Proof of Transformation Chain

### Theorem: Composition of Transformations

**Given:**
- Point **p** in GelSight frame
- Want to express **p** in robot base frame

**Prove:**
```
p_base = T_{baseâ†’ee} Â· T_{eeâ†’cam} Â· T_{camâ†’gel} Â· p_gel
```

### Proof by Construction

#### Step 1: Point in GelSight Frame to Camera Frame

A point **p_gel** in GelSight coordinates can be expressed in camera coordinates:

```
p_cam = T_{camâ†’gel} Â· p_gel
```

Expanded:
```
[x_cam]   [R_{camâ†’gel}  |  t_{camâ†’gel}]   [x_gel]
[y_cam] = [             |              ] Â· [y_gel]
[z_cam]   [    0áµ€       |      1       ]   [z_gel]
[ 1   ]                                     [ 1   ]
```

**What this means physically:**
- R_{camâ†’gel} rotates the point from GelSight orientation to camera orientation
- t_{camâ†’gel} translates the point from GelSight origin to camera origin
- Result: Point coordinates in camera frame

**Derivation:**
```
[x_cam]   [R_{camâ†’gel}] [x_gel]   [t_{camâ†’gel}]
[y_cam] = [           ] [y_gel] + [           ]
[z_cam]   [           ] [z_gel]   [           ]

Position in camera = (Rotation applied to gel position) + (translation from gel to cam)
```

---

#### Step 2: Point in Camera Frame to End-Effector Frame

The same point in end-effector coordinates:

```
p_ee = T_{eeâ†’cam} Â· p_cam
```

Substituting p_cam from Step 1:
```
p_ee = T_{eeâ†’cam} Â· (T_{camâ†’gel} Â· p_gel)
```

Expanded:
```
[x_ee]   [R_{eeâ†’cam}  |  t_{eeâ†’cam}]   [R_{camâ†’gel}  |  t_{camâ†’gel}]   [x_gel]
[y_ee] = [            |             ] Â· [             |              ] Â· [y_gel]
[z_ee]   [    0áµ€      |      1      ]   [    0áµ€       |      1       ]   [z_gel]
[ 1  ]                                                                    [ 1   ]
```

**What this means physically:**
- First transform moves point from GelSight to camera frame
- Second transform moves point from camera to end-effector frame
- Result: Point coordinates in end-effector frame

**Matrix multiplication (T_{eeâ†’cam} Â· T_{camâ†’gel}):**

Using the multiplication rule:
```
T_{eeâ†’gel} = [R_{eeâ†’cam}Â·R_{camâ†’gel}  |  R_{eeâ†’cam}Â·t_{camâ†’gel} + t_{eeâ†’cam}]
             [         0áµ€              |              1                      ]
```

Let's verify the translation component:
```
t_{eeâ†’gel} = R_{eeâ†’cam}Â·t_{camâ†’gel} + t_{eeâ†’cam}

Physical meaning:
- t_{camâ†’gel}: Vector from camera origin to GelSight origin (in camera frame)
- R_{eeâ†’cam}Â·t_{camâ†’gel}: Same vector rotated to ee frame orientation
- + t_{eeâ†’cam}: Add vector from ee origin to camera origin
- Result: Total vector from ee origin to GelSight origin
```

---

#### Step 3: Point in End-Effector Frame to Base Frame

Finally, express the point in base coordinates:

```
p_base = T_{baseâ†’ee} Â· p_ee
```

Substituting p_ee from Step 2:
```
p_base = T_{baseâ†’ee} Â· (T_{eeâ†’cam} Â· T_{camâ†’gel} Â· p_gel)
```

By associativity of matrix multiplication:
```
p_base = (T_{baseâ†’ee} Â· T_{eeâ†’cam} Â· T_{camâ†’gel}) Â· p_gel
```

Let:
```
T_{baseâ†’gel} = T_{baseâ†’ee} Â· T_{eeâ†’cam} Â· T_{camâ†’gel}
```

Then:
```
p_base = T_{baseâ†’gel} Â· p_gel
```

**This is our final transformation chain!** âˆ

---

### Explicit Formula Derivation

#### Rotation Component

Starting from:
```
T_{baseâ†’gel} = T_{baseâ†’ee} Â· T_{eeâ†’cam} Â· T_{camâ†’gel}
```

First multiply T_{eeâ†’cam} Â· T_{camâ†’gel}:
```
Step A: R_{eeâ†’gel} = R_{eeâ†’cam} Â· R_{camâ†’gel}
```

Then multiply T_{baseâ†’ee} with the result:
```
Step B: R_{baseâ†’gel} = R_{baseâ†’ee} Â· R_{eeâ†’gel}
                     = R_{baseâ†’ee} Â· (R_{eeâ†’cam} Â· R_{camâ†’gel})
```

By associativity:
```
R_{baseâ†’gel} = R_{baseâ†’ee} Â· R_{eeâ†’cam} Â· R_{camâ†’gel}
```

**Physical interpretation:**
- Each rotation matrix represents change of orientation between frames
- Composition means: "First rotate by R_{camâ†’gel}, then by R_{eeâ†’cam}, then by R_{baseâ†’ee}"
- Order matters! Matrix multiplication is not commutative.

---

#### Translation Component

For translations, we use the multiplication rule stepwise:

**Step A:** Multiply T_{eeâ†’cam} Â· T_{camâ†’gel}:
```
t_{eeâ†’gel} = R_{eeâ†’cam} Â· t_{camâ†’gel} + t_{eeâ†’cam}
```

**Why?**
- t_{camâ†’gel} is in camera coordinates
- Must rotate it to ee coordinates: R_{eeâ†’cam} Â· t_{camâ†’gel}
- Then add the ee-to-camera offset: + t_{eeâ†’cam}

**Step B:** Multiply T_{baseâ†’ee} Â· T_{eeâ†’gel}:
```
t_{baseâ†’gel} = R_{baseâ†’ee} Â· t_{eeâ†’gel} + t_{baseâ†’ee}
```

Substituting t_{eeâ†’gel} from Step A:
```
t_{baseâ†’gel} = R_{baseâ†’ee} Â· (R_{eeâ†’cam} Â· t_{camâ†’gel} + t_{eeâ†’cam}) + t_{baseâ†’ee}
```

Distributing:
```
t_{baseâ†’gel} = R_{baseâ†’ee}Â·R_{eeâ†’cam}Â·t_{camâ†’gel} + R_{baseâ†’ee}Â·t_{eeâ†’cam} + t_{baseâ†’ee}
```

**Final formulas:**
```
Rotation:
R_{baseâ†’gel} = R_{baseâ†’ee} Â· R_{eeâ†’cam} Â· R_{camâ†’gel}

Translation:
t_{baseâ†’gel} = t_{baseâ†’ee} + R_{baseâ†’ee}Â·t_{eeâ†’cam} + R_{baseâ†’ee}Â·R_{eeâ†’cam}Â·t_{camâ†’gel}
```

Or compactly:
```
t_{baseâ†’gel} = t_{baseâ†’ee} + R_{baseâ†’ee}Â·(t_{eeâ†’cam} + R_{eeâ†’cam}Â·t_{camâ†’gel})
```

---

### Verification: What Comes From What

#### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUTS (What We Obtain)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Robot End-Effector Pose:                                â”‚
â”‚     Output: T_{baseâ†’ee} = [R_{baseâ†’ee} | t_{baseâ†’ee}]      â”‚
â”‚             (Cartesian 6DOF: x, y, z, roll, pitch, yaw)     â”‚
â”‚     Source: Robot API (driver computes FK internally)       â”‚
â”‚                                                             â”‚
â”‚  2. Hand-Eye Calibration (Camera-to-EE transform):          â”‚
â”‚     Output: X = T_{eeâ†’camera} = [R_{eeâ†’cam} | t_{eeâ†’cam}]  â”‚
â”‚     Source: MoveIt Calibration library                      â”‚
â”‚             (uses ArUco marker + multiple robot poses)      â”‚
â”‚                                                             â”‚
â”‚  3. Camera-to-GelSight Calibration:                         â”‚
â”‚     Output: T_{cameraâ†’gelsight} = [R_{camâ†’gel} | t_{camâ†’gel}]â”‚
â”‚     Source: Custom script using camera image of GelSight    â”‚
â”‚             sensor mounted on end-effector                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINAL COMPUTATION (What We Want)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  T_{baseâ†’gelsight} = T_{baseâ†’ee} Â· X Â· T_{cameraâ†’gelsight} â”‚
â”‚                                                             â”‚
â”‚  Components:                                                â”‚
â”‚  R_{baseâ†’gel} = R_{baseâ†’ee} Â· R_{eeâ†’cam} Â· R_{camâ†’gel}     â”‚
â”‚                      â†‘              â†‘             â†‘         â”‚
â”‚                 (Robot API)   (MoveIt)    (Custom script)  â”‚
â”‚                                                             â”‚
â”‚  t_{baseâ†’gel} = t_{baseâ†’ee} + R_{baseâ†’ee}Â·t_{eeâ†’cam}       â”‚
â”‚                 + R_{baseâ†’ee}Â·R_{eeâ†’cam}Â·t_{camâ†’gel}        â”‚
â”‚                      â†‘              â†‘             â†‘         â”‚
â”‚                 (Robot API)   (MoveIt)    (Custom script)  â”‚
â”‚                                                             â”‚
â”‚  Result: GelSight position [x, y, z] in robot base frame   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Summary: Source of Each Component

| Component | What It Is | Source Method | Tool |
|-----------|------------|---------------|------|
| **R_{baseâ†’ee}** | Gripper rotation in base | Robot API (Cartesian 6DOF) | Robot driver |
| **t_{baseâ†’ee}** | Gripper position in base | Robot API (Cartesian 6DOF) | Robot driver |
| **R_{eeâ†’cam}** | Camera rotation in gripper | Hand-eye calibration | MoveIt Calibration |
| **t_{eeâ†’cam}** | Camera position in gripper | Hand-eye calibration | MoveIt Calibration |
| **R_{camâ†’gel}** | GelSight rotation in camera | Camera-to-GelSight calibration | Custom script |
| **t_{camâ†’gel}** | GelSight position in camera | Camera-to-GelSight calibration | Custom script |
| **R_{baseâ†’gel}** | GelSight rotation in base | **Computed:** Râ‚Â·Râ‚‚Â·Râ‚ƒ | Matrix multiplication |
| **t_{baseâ†’gel}** | GelSight position in base | **Computed:** formula above | Matrix multiplication |

**Key insight:**
- T_{baseâ†’ee}: From robot API directly (Cartesian 6DOF)
- T_{eeâ†’camera} (X): From MoveIt Calibration library (hand-eye calibration)
- T_{cameraâ†’gelsight}: From custom script using camera image of GelSight on EE
- Final result: Computed by chaining the three transformations

---

## How We Obtain Each Transform

### 1. T_{baseâ†’ee} (Robot Base to End-Effector)

#### What it is:
```
T_{baseâ†’ee} = [R_{baseâ†’ee}  |  t_{baseâ†’ee}]
              [    0áµ€       |      1      ]
```
- **R_{baseâ†’ee}** = Gripper orientation in base frame (3Ã—3 rotation matrix)
- **t_{baseâ†’ee}** = Gripper position in base frame (3Ã—1 vector [x, y, z]áµ€)

#### How we get it:
**Directly from the robot API in Cartesian 6DOF format** (robot driver computes FK internally)

```python
# Robot API call - returns Cartesian pose directly
ee_pose = robot.get_ee_pose()
# Returns: [x, y, z, roll, pitch, yaw] in Cartesian coordinates

# The robot driver internally computes:
#   1. Reads joint encoders [Î¸â‚, Î¸â‚‚, ..., Î¸â‚†]
#   2. Applies forward kinematics using robot's kinematic model
#   3. Returns end-effector pose in Cartesian 6DOF
```

#### Converting 6DOF to 4Ã—4 Transformation Matrix

Given pose = [x, y, z, roll (Ï†), pitch (Î¸), yaw (Ïˆ)]:

**Translation vector:**
```
t = [x, y, z]áµ€
```

**Rotation matrix (ZYX Euler angles convention):**
```
R = Rz(Ïˆ) Â· Ry(Î¸) Â· Rx(Ï†)
```

Where the individual rotation matrices are:

```
Rx(Ï†) = [1      0       0   ]      Ry(Î¸) = [ cos(Î¸)  0  sin(Î¸)]      Rz(Ïˆ) = [cos(Ïˆ)  -sin(Ïˆ)  0]
        [0   cos(Ï†)  -sin(Ï†)]              [   0     1    0   ]              [sin(Ïˆ)   cos(Ïˆ)  0]
        [0   sin(Ï†)   cos(Ï†)]              [-sin(Î¸)  0  cos(Î¸)]              [  0        0     1]
```

**Combined rotation matrix R = Rz(Ïˆ) Â· Ry(Î¸) Â· Rx(Ï†):**
```
R = [cos(Ïˆ)cos(Î¸)   cos(Ïˆ)sin(Î¸)sin(Ï†)-sin(Ïˆ)cos(Ï†)   cos(Ïˆ)sin(Î¸)cos(Ï†)+sin(Ïˆ)sin(Ï†)]
    [sin(Ïˆ)cos(Î¸)   sin(Ïˆ)sin(Î¸)sin(Ï†)+cos(Ïˆ)cos(Ï†)   sin(Ïˆ)sin(Î¸)cos(Ï†)-cos(Ïˆ)sin(Ï†)]
    [  -sin(Î¸)              cos(Î¸)sin(Ï†)                       cos(Î¸)cos(Ï†)            ]
```

**Final 4Ã—4 homogeneous transformation matrix:**
```
T_{baseâ†’ee} = [R  |  t]  =  [râ‚â‚  râ‚â‚‚  râ‚â‚ƒ  x]
              [0áµ€ |  1]     [râ‚‚â‚  râ‚‚â‚‚  râ‚‚â‚ƒ  y]
                            [râ‚ƒâ‚  râ‚ƒâ‚‚  râ‚ƒâ‚ƒ  z]
                            [ 0    0    0   1]
```

**Source:** Robot API call (forward kinematics computed internally by robot driver)

**Note:** You don't need to compute FK yourself - the robot driver handles this and returns the Cartesian pose directly.

**Accuracy:** High (Â±0.1mm) - robot knows its own position well

---

### 2. T_{eeâ†’camera} (End-Effector to Camera) = X

#### What it is:
```
X = T_{eeâ†’camera} = [R_{eeâ†’cam}  |  t_{eeâ†’cam}]
                    [    0áµ€      |      1     ]
```
- **R_{eeâ†’cam}** = Camera orientation relative to gripper (3Ã—3 rotation matrix)
- **t_{eeâ†’cam}** = Camera position relative to gripper (3Ã—1 vector)

#### How we get it:
**MoveIt Calibration Library (Hand-Eye Calibration)**

**IMPORTANT:** This calibration is done entirely by MoveIt Calibration library. Without MoveIt, this calibration cannot be performed!

##### What MoveIt Calibration Does:

MoveIt Calibration is a complete hand-eye calibration solution that handles:
1. **ArUco marker detection** - Detects the marker in camera images automatically
2. **Robot pose collection** - Gets T_{baseâ†’ee} from robot at each position
3. **Data pairing** - Collects (robot pose, marker pose) pairs at multiple positions
4. **AX=XB solving** - Solves the hand-eye calibration equation internally
5. **Result output** - Outputs T_{eeâ†’camera} transform

##### The AX=XB Equation (solved internally by MoveIt):

For any two robot poses i and j:
```
A_ij Ã— X = X Ã— B_ij

Where:
A_ij = (T_{baseâ†’ee}^j)â»Â¹ Â· T_{baseâ†’ee}^i  (robot motion from pose i to j)
B_ij = (T_{cameraâ†’marker}^j)â»Â¹ Â· T_{cameraâ†’marker}^i  (observed camera motion)
X = T_{eeâ†’camera}  (what MoveIt solves for)
```

**Key insight:** The marker is fixed in the world. When robot moves, both the robot motion (A) and observed camera motion (B) must be consistent through X.

##### Experiment Setup:

```
Physical Setup:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚    [ArUco Marker]  â† Fixed on table, does NOT move     â”‚
â”‚          â†‘                                              â”‚
â”‚          â”‚ Camera sees marker                           â”‚
â”‚          â”‚                                              â”‚
â”‚    [RealSense Camera] â† Mounted on gripper             â”‚
â”‚          â”‚                                              â”‚
â”‚    [Robot Gripper/EE]                                  â”‚
â”‚          â”‚                                              â”‚
â”‚    [Robot Arm]                                          â”‚
â”‚          â”‚                                              â”‚
â”‚    [Robot Base]                                         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### MoveIt Calibration Workflow:

```
Step 1: Setup
   - Fix ArUco marker on table (must NOT move during calibration!)
   - Mount RealSense camera on robot gripper
   - Launch MoveIt Calibration:
     â†’ ros2 launch moveit_calibration hand_eye_calibration.launch.py

Step 2: Data Collection (15-30 poses)
   - Move robot to position where camera sees marker
   - MoveIt automatically:
     â†’ Detects ArUco marker in camera image
     â†’ Records T_{cameraâ†’marker} (marker pose in camera frame)
     â†’ Records T_{baseâ†’ee} (gripper pose from robot API)
     â†’ Stores the pair
   - Repeat at diverse positions with different:
     â†’ Distances from marker (30-60 cm)
     â†’ Viewing angles (0-60Â° off-axis)
     â†’ Robot orientations (roll, pitch, yaw variations)

Step 3: Solve (automatic)
   - After collecting enough samples, MoveIt solves AX=XB
   - Uses optimization to find best X that satisfies all pose pairs

Step 4: Output
   - MoveIt outputs: X = T_{eeâ†’camera}
   - Save to calibration file for later use
```

##### Why Diverse Poses Matter:

```
Good poses (maximize information):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Position 1      Position 2      Position 3             â”‚
â”‚     â•±               â”‚               â•²                   â”‚
â”‚    â•±                â”‚                â•²                  â”‚
â”‚   ğŸ“·               ğŸ“·               ğŸ“·  â† Different anglesâ”‚
â”‚                                                          â”‚
â”‚              [ArUco Marker]                              â”‚
â”‚                                                          â”‚
â”‚  Position 4      Position 5      Position 6             â”‚
â”‚     ğŸ“·              ğŸ“·              ğŸ“·   â† Different distancesâ”‚
â”‚      â†‘               â†‘               â†‘                  â”‚
â”‚     far           medium          close                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Bad poses (insufficient information):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“· ğŸ“· ğŸ“· ğŸ“· ğŸ“· ğŸ“·  â† All same angle, same distance      â”‚
â”‚        â†“                                                 â”‚
â”‚  [ArUco Marker]                                          â”‚
â”‚                                                          â”‚
â”‚  Result: Poor calibration, high error!                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Source:** MoveIt Calibration library (handles everything internally)

**Accuracy:** Â±0.3-0.5mm when done with diverse poses

---

### 3. T_{cameraâ†’gelsight} (Camera to GelSight Center)

#### What it is:
```
T_{camâ†’gel} = [R_{camâ†’gel}  |  t_{camâ†’gel}]
              [    0áµ€       |      1      ]
```
- **R_{camâ†’gel}** = GelSight orientation relative to camera (3Ã—3 rotation matrix)
- **t_{camâ†’gel}** = GelSight position relative to camera (3Ã—1 vector)

#### How we get it:
**Custom Script using Camera Image of GelSight Sensor**

This calibration is done by our own script that captures an image of the GelSight sensor (which is mounted on the end-effector) using the RealSense camera (also on the end-effector).

##### Physical Setup:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  END-EFFECTOR ASSEMBLY                                      â”‚
â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚     â”‚  RealSense  â”‚ â† Camera (captures image)              â”‚
â”‚     â”‚   Camera    â”‚                                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚            â”‚                                                â”‚
â”‚            â”‚ Camera looks at GelSight                       â”‚
â”‚            â†“                                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚     â”‚  GelSight   â”‚ â† Tactile sensor (visible in image)    â”‚
â”‚     â”‚   Sensor    â”‚                                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚            â”‚                                                â”‚
â”‚     [Robot Gripper]                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key: Both camera and GelSight are rigidly mounted on the gripper.
     Their relative position is FIXED and does not change.
```

##### Why This Works:

Since both the RealSense camera and GelSight sensor are mounted on the same rigid body (the gripper), their relative transform T_{cameraâ†’gelsight} is **constant**. We only need to measure it once!

##### Experiment Procedure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Position the Robot                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Move robot to a position where the RealSense camera      â”‚
â”‚   can clearly see the GelSight sensor surface.             â”‚
â”‚                                                             â”‚
â”‚   This may require:                                         â”‚
â”‚   - Using a mirror to reflect the GelSight into camera viewâ”‚
â”‚   - OR temporarily detaching camera to image GelSight      â”‚
â”‚   - OR using a second external camera                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Capture Image                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Capture RGB image from RealSense camera showing the      â”‚
â”‚   GelSight sensor clearly visible in the frame.            â”‚
â”‚                                                             â”‚
â”‚   Image should show:                                        â”‚
â”‚   - GelSight sensing surface (rectangular area)            â”‚
â”‚   - Clear corners or identifiable features                 â”‚
â”‚   - Good lighting, no blur                                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Identify GelSight Features in Image                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   In the captured image, identify known points on GelSight:â”‚
â”‚                                                             â”‚
â”‚   Option A: Click 4 corners of GelSight sensing surface    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚   â”‚ â€¢               â€¢   â”‚  â† Click corners in image        â”‚
â”‚   â”‚                     â”‚                                   â”‚
â”‚   â”‚                     â”‚                                   â”‚
â”‚   â”‚ â€¢               â€¢   â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                             â”‚
â”‚   Option B: Use ArUco marker attached to GelSight          â”‚
â”‚   (if marker is placed on GelSight housing)                â”‚
â”‚                                                             â”‚
â”‚   Option C: Detect GelSight edges automatically            â”‚
â”‚   (using edge detection algorithms)                         â”‚
â”‚                                                             â”‚
â”‚   Result: 2D pixel coordinates [(uâ‚,vâ‚), (uâ‚‚,vâ‚‚), ...]     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Define 3D Points (from GelSight Dimensions)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   From GelSight datasheet, we know the physical dimensions:â”‚
â”‚                                                             â”‚
â”‚   Sensing area: 18.6mm (width) Ã— 14.3mm (height)           â”‚
â”‚                                                             â”‚
â”‚   Define 3D coordinates in GelSight frame (center = origin)â”‚
â”‚                                                             â”‚
â”‚   Corner 1 (top-left):     (+9.3mm, +7.15mm, 0)            â”‚
â”‚   Corner 2 (top-right):    (-9.3mm, +7.15mm, 0)            â”‚
â”‚   Corner 3 (bottom-right): (-9.3mm, -7.15mm, 0)            â”‚
â”‚   Corner 4 (bottom-left):  (+9.3mm, -7.15mm, 0)            â”‚
â”‚                                                             â”‚
â”‚   Note: Z=0 means corners lie on the GelSight surface planeâ”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Solve PnP (Perspective-n-Point)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   PnP Problem:                                              â”‚
â”‚   Given: - 3D points in GelSight frame (from dimensions)   â”‚
â”‚          - 2D points in image (from Step 3)                â”‚
â”‚          - Camera intrinsics (from camera calibration)     â”‚
â”‚   Find:  - T_{cameraâ†’gelsight}                             â”‚
â”‚                                                             â”‚
â”‚   Mathematical formulation:                                 â”‚
â”‚                                                             â”‚
â”‚   For each point i:                                         â”‚
â”‚                                                             â”‚
â”‚   [u_i]       [p_i^gel]                                     â”‚
â”‚   [v_i] = K Â· T_{camâ†’gel} Â· [  1  ]                        â”‚
â”‚   [ 1 ]                                                     â”‚
â”‚                                                             â”‚
â”‚   Where:                                                    â”‚
â”‚   - (u_i, v_i) = pixel coordinates                         â”‚
â”‚   - K = camera intrinsic matrix (3Ã—3)                      â”‚
â”‚   - T_{camâ†’gel} = transform we want to find (4Ã—4)         â”‚
â”‚   - p_i^gel = 3D point in GelSight frame                   â”‚
â”‚                                                             â”‚
â”‚   PnP solver finds R and t that minimize reprojection errorâ”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Output Transform                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   PnP solver outputs:                                       â”‚
â”‚   - rvec: rotation vector (3Ã—1)                            â”‚
â”‚   - tvec: translation vector (3Ã—1)                         â”‚
â”‚                                                             â”‚
â”‚   Convert to transformation matrix:                         â”‚
â”‚                                                             â”‚
â”‚   R_{camâ†’gel} = rodrigues(rvec)   (3Ã—3 rotation matrix)   â”‚
â”‚   t_{camâ†’gel} = tvec              (3Ã—1 translation)        â”‚
â”‚                                                             â”‚
â”‚   T_{camâ†’gel} = [R_{camâ†’gel}  |  t_{camâ†’gel}]             â”‚
â”‚                 [    0áµ€       |      1      ]              â”‚
â”‚                                                             â”‚
â”‚   Save to calibration file for later use.                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### Required Inputs:

| Input | Source | Description |
|-------|--------|-------------|
| Camera image | RealSense camera | Image showing GelSight sensor |
| 2D pixel coordinates | Manual click or detection | Corners of GelSight in image |
| 3D GelSight dimensions | Datasheet | Physical size of sensing area |
| Camera intrinsics (K) | Camera calibration | Focal length, principal point |
| Distortion coefficients | Camera calibration | Lens distortion parameters |

##### Camera Intrinsic Matrix K:

```
K = [fx   0  cx]
    [ 0  fy  cy]
    [ 0   0   1]

Where:
- fx, fy = focal lengths in pixels
- cx, cy = principal point (image center)
```

##### PnP Reprojection Error:

The solver minimizes:
```
E = Î£áµ¢ || (u_i, v_i) - project(T_{camâ†’gel} Â· p_i^gel) ||Â²

Where project() applies camera projection:
project(P) = K Â· [P_x/P_z, P_y/P_z, 1]áµ€
```

**Source:** Custom script (camera image + PnP solver)

**Accuracy:** Â±0.5-1mm (depends on corner detection accuracy and camera calibration quality)

**Note:** This calibration only needs to be done ONCE since the camera and GelSight are rigidly mounted together

---

### 4. T_{baseâ†’gelsight} (Base to GelSight Center) - FINAL RESULT

#### What it is:
```
T_{baseâ†’gel} = [R_{baseâ†’gel}  |  t_{baseâ†’gel}]
               [    0áµ€        |      1       ]
```
- **R_{baseâ†’gel}** = GelSight orientation in base frame (3Ã—3 rotation matrix)
- **t_{baseâ†’gel}** = GelSight position in base frame (3Ã—1 vector [x, y, z]áµ€)

#### How we compute it:
**Matrix multiplication of the three transforms above**

```
T_{baseâ†’gel} = T_{baseâ†’ee} Ã— X Ã— T_{camâ†’gel}
```

##### Expanded form:
```
[R_{baseâ†’gel}  |  t_{baseâ†’gel}]   [R_{baseâ†’ee}  |  t_{baseâ†’ee}]   [R_{eeâ†’cam}  |  t_{eeâ†’cam}]   [R_{camâ†’gel}  |  t_{camâ†’gel}]
[    0áµ€        |      1       ] = [    0áµ€       |      1      ] Â· [    0áµ€     |      1     ] Â· [    0áµ€       |      1      ]
```

##### Component formulas:

**Rotation:**
```
R_{baseâ†’gel} = R_{baseâ†’ee} Â· R_{eeâ†’cam} Â· R_{camâ†’gel}
```

**Translation:**
```
t_{baseâ†’gel} = t_{baseâ†’ee} + R_{baseâ†’ee}Â·t_{eeâ†’cam} + R_{baseâ†’ee}Â·R_{eeâ†’cam}Â·t_{camâ†’gel}
```

Or more compactly:
```
t_{baseâ†’gel} = t_{baseâ†’ee} + R_{baseâ†’ee}Â·(t_{eeâ†’cam} + R_{eeâ†’cam}Â·t_{camâ†’gel})
```

**This gives us the 3D position [x, y, z] and orientation (rotation matrix) of the GelSight sensor center in robot base coordinates!**

---

## The Complete Workflow

### Phase 1: Calibrations (One-Time Setup)

#### Step 1: Camera Intrinsic Calibration
**Goal:** Get camera matrix and distortion coefficients

**Method:** Standard checkerboard calibration
```
1. Print checkerboard pattern
2. Capture 20+ images of checkerboard at different angles
3. Use OpenCV camera calibration
4. Get: camera_matrix, dist_coeffs
```

**Tool:** OpenCV `cv2.calibrateCamera()`

**Output:** Camera intrinsics file (used for all subsequent CV operations)

---

#### Step 2: Hand-Eye Calibration (Get X = T_{eeâ†’camera})
**Goal:** Find camera position and orientation relative to gripper

**Method:** AX=XB calibration with ArUco marker

**Tools:**
- ROS 2 + MoveIt 2 (motion planning and robot control)
- OpenCV (ArUco marker detection)
- MoveIt Calibration GUI or custom script

**Detailed Process:**

1. **Setup:**
   ```
   - Fix ArUco marker to table (don't move it!)
   - Start ROS 2 robot driver
   - Launch MoveIt
   - Launch camera node
   ```

2. **Data Collection:**
   ```
   For 15-30 diverse poses:
       a) Move robot to pose where camera sees marker
          (Use MoveIt GUI or motion planning)

       b) Detect ArUco marker in camera image
          â†’ Get T_{cameraâ†’marker} (from CV)

       c) Get robot pose
          â†’ Get T_{baseâ†’ee} (from robot FK)

       d) Save pair: (T_{baseâ†’ee}, T_{cameraâ†’marker})
   ```

3. **Diversity Requirements:**
   ```
   Good calibration needs diverse poses:
   - Different distances from marker (30-60cm)
   - Different viewing angles (0-60Â° off-axis)
   - Rotations around all axes (roll, pitch, yaw)
   ```

4. **Solve AX=XB:**
   ```python
   # Input: List of (T_{baseâ†’ee}, T_{cameraâ†’marker}) pairs
   R_gripper2cam, t_gripper2cam = cv2.calibrateHandEye(
       R_base2gripper,  # List of rotation matrices
       t_base2gripper,  # List of translation vectors
       R_cam2marker,    # List of rotation matrices
       t_cam2marker,    # List of translation vectors
       method=cv2.CALIB_HAND_EYE_TSAI
   )
   ```

5. **Validation:**
   ```
   - Check reprojection error
   - Move to new poses and verify marker position
   - Should be <2 pixels error
   ```

6. **Save Result:**
   ```
   X = T_{eeâ†’camera} = [R_{eeâ†’cam} | t_{eeâ†’cam}]
   Save to: hand_eye_calibration.json
   ```

**Output:** X = T_{eeâ†’camera} (camera pose in gripper frame)

**Accuracy:** Â±0.3-0.5mm (if done properly)

---

#### Step 3: Camera-to-GelSight Calibration (Get T_{cameraâ†’gelsight})
**Goal:** Find GelSight sensor position and orientation relative to camera

**Method:** PnP with GelSight 4 corners

**Tools:**
- OpenCV (PnP solver)
- Camera image

**Detailed Process:**

1. **Get GelSight Corner Positions (from datasheet):**
   ```
   Field of View: 18.6mm Ã— 14.3mm

   3D positions in GelSight frame (meters):
   corner1 = [ 0.0093,  0.00715, 0.0]  # Top-left
   corner2 = [-0.0093,  0.00715, 0.0]  # Top-right
   corner3 = [-0.0093, -0.00715, 0.0]  # Bottom-right
   corner4 = [ 0.0093, -0.00715, 0.0]  # Bottom-left
   ```

2. **Capture Image:**
   ```
   - Position robot so camera sees GelSight clearly
   - Capture RGB image
   ```

3. **Detect Corners in Image:**
   ```
   Option A: Manual clicking
   - Click 4 corners in order
   - Get pixel coordinates [(u1,v1), (u2,v2), (u3,v3), (u4,v4)]

   Option B: Automatic detection
   - Use corner detection algorithm
   - Or detect visual markers on GelSight
   ```

4. **Solve PnP:**
   ```python
   success, rvec, tvec = cv2.solvePnP(
       corners_3d,      # From datasheet
       corners_2d,      # From image
       camera_matrix,   # From camera calibration
       dist_coeffs      # From camera calibration
   )

   R_{camâ†’gel}, _ = cv2.Rodrigues(rvec)
   t_{camâ†’gel} = tvec
   ```

5. **Save Result:**
   ```
   T_{cameraâ†’gelsight} = [R_{camâ†’gel} | t_{camâ†’gel}]
   Save to: camera_to_gelsight.json
   ```

**Output:** T_{cameraâ†’gelsight} (GelSight pose in camera frame)

**Accuracy:** Â±0.5-1mm

---

### Phase 2: Data Collection (Repeated for Each Sample)

#### During GelSight Data Collection:

For each touch sample:

1. **Robot moves and GelSight touches object**
   ```
   - Robot executes motion to touch object
   - GelSight makes contact with surface
   ```

2. **Capture data:**
   ```
   - gelsight_image = GelSight tactile image
   - camera_image = RealSense RGB image
   - timestamp = Current time
   ```

3. **Get robot pose:**
   ```python
   ee_pose = robot.get_ee_pose()  # [x, y, z, roll, pitch, yaw]
   T_{baseâ†’ee} = pose_to_matrix(ee_pose)
   ```

4. **Load calibrations:**
   ```python
   X = load("hand_eye_calibration.json")  # T_{eeâ†’camera}
   T_{camâ†’gel} = load("camera_to_gelsight.json")
   ```

5. **Compute GelSight position in base frame:**
   ```python
   T_{baseâ†’gel} = T_{baseâ†’ee} @ X @ T_{camâ†’gel}

   # Extract position and orientation
   gelsight_position = T_{baseâ†’gel}[0:3, 3]  # [x, y, z]
   gelsight_orientation = T_{baseâ†’gel}[0:3, 0:3]  # Rotation matrix
   ```

6. **Compute 4 corners of GelSight in base frame:**
   ```python
   corners_relative = [
       [ 0.0093,  0.00715, 0, 1],  # Homogeneous coordinates
       [-0.0093,  0.00715, 0, 1],
       [-0.0093, -0.00715, 0, 1],
       [ 0.0093, -0.00715, 0, 1]
   ]

   corners_in_base = []
   for corner in corners_relative:
       corner_base = T_{baseâ†’gel} @ corner
       corners_in_base.append(corner_base[0:3])
   ```

7. **Save complete data sample:**
   ```json
   {
     "gelsight_image": "frame_001_tactile.png",
     "camera_image": "frame_001_rgb.png",
     "timestamp": "2025-01-19T10:30:45.123Z",
     "robot_pose": {
       "position": [x, y, z],
       "orientation": [roll, pitch, yaw]
     },
     "gelsight_center": {
       "position": [x, y, z],
       "orientation_matrix": [[r11, r12, r13], ...]
     },
     "gelsight_corners": [
       [x1, y1, z1],
       [x2, y2, z2],
       [x3, y3, z3],
       [x4, y4, z4]
     ]
   }
   ```

---

## Tools and Methods Summary

### Transform Sources

| Transform | Source | Tool/Method |
|-----------|--------|-------------|
| T_{baseâ†’ee} | Robot API | `robot.get_ee_pose()` returns Cartesian 6DOF |
| T_{eeâ†’camera} (X) | Hand-eye calibration | MoveIt Calibration library |
| T_{cameraâ†’gelsight} | Camera-to-GelSight calibration | Custom script (camera image of GelSight on EE) |

### MoveIt Calibration Role

MoveIt Calibration library handles the complete hand-eye calibration:
- **ArUco marker detection:** Detects marker in camera images
- **Data collection:** Collects (robot pose, marker pose) pairs at multiple positions
- **AX=XB solver:** Solves the hand-eye calibration equation internally
- **Output:** T_{eeâ†’camera} transform

### Robot API Role

The robot driver provides end-effector pose directly:
- **Input:** API call to robot driver
- **Output:** Cartesian 6DOF (x, y, z, roll, pitch, yaw)
- **Note:** FK is computed internally by the driver, no manual computation needed

---

## Why This Matters for YCB-Sight Dataset

For each GelSight tactile sample, researchers need to know:

1. âœ… **What was felt** (GelSight tactile image)
2. âœ… **What was seen** (RealSense camera image)
3. âœ… **Where it was felt** (3D position on object) â† **Requires this calibration!**
4. âœ… **Surface geometry** (normal vector, curvature) â† **Requires this calibration!**
5. âœ… **Alignment** (tactile-visual correspondence) â† **Requires this calibration!**

**Without accurate calibration, the dataset lacks precise 3D geometry information, making it less useful for learning tactile-visual relationships!**

---

## References

### Algorithms
- **AX=XB Calibration:** Daniilidis, "Hand-Eye Calibration Using Dual Quaternions" (1999)
- **Alternative methods:** Tsai-Lenz, Park, Horaud, Andreff

### Tools
- **ROS 2:** https://docs.ros.org/
- **MoveIt 2:** https://moveit.ros.org/
- **MoveIt Calibration:** https://github.com/moveit/moveit_calibration
- **OpenCV:** https://opencv.org/

### Your Implementation
- Code location: `/home/skhan61/Desktop/trossen-arm-data-collection/src/`
- Hand-eye calibration script: `src/hand_eye_calibration.py`
- Calibration computation: `src/compute_hand_eye.py`

---

**Document created:** 2026-01-19
**For project:** GelSight Tactile Data Collection with WidowX Robot
